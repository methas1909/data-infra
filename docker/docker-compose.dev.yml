services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    extra_hosts:
      - server:127.0.0.1

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    depends_on:
      - zookeeper
    ports:
      - '9092:9092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    extra_hosts:
      - server:127.0.0.1

  minio:
    image: minio/minio:latest
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    extra_hosts:
      - server:127.0.0.1

  minio-setup:
    image: minio/mc:latest
    restart: on-failure
    depends_on:
      - minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET_RAW: ${MINIO_BUCKET_RAW}
      MINIO_BUCKET_CLEAN: ${MINIO_BUCKET_CLEAN}
      MINIO_BUCKET_MART: ${MINIO_BUCKET_MART}
    entrypoint: |
      sh -c " until (/usr/bin/mc alias set local http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}); do sleep 2; done && /usr/bin/mc mb -p local/${MINIO_BUCKET_RAW} || true && /usr/bin/mc mb -p local/${MINIO_BUCKET_CLEAN} || true && /usr/bin/mc mb -p local/${MINIO_BUCKET_MART} || true && echo 'MinIO buckets ensured' "

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    ports:
      - '8123:8123'
      - '9005:9000'
    volumes:
      - ch_data:/var/lib/clickhouse
      - ./scripts/ch_init.sql:/docker-entrypoint-initdb.d/ch_init.sql:ro
    environment:
      CLICKHOUSE_DB: ${CH_DB}
      CLICKHOUSE_USER: ${CH_USER}
      CLICKHOUSE_PASSWORD: ${CH_PASSWORD}
    extra_hosts:
      - server:127.0.0.1

  airflow:
    image: apache/airflow:2.9.3
    restart: unless-stopped
    ports:
      - '8080:8080'
    volumes:
      - airflow_logs:/opt/airflow/logs
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__WEBSERVER__WEB_SERVER_HOST: 0.0.0.0
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      _PIP_ADDITIONAL_REQUIREMENTS: |
        apache-airflow-providers-amazon
        clickhouse-connect
        apache-airflow-providers-apache-spark==4.2.0
        pyspark==3.5.1
        pandas
        pyopenssl
        polars==0.20.31
        pyarrow
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: us-east-1
      AIRFLOW_GENERATE_FERNET_ON_START: ${AIRFLOW_GENERATE_FERNET_ON_START}
    healthcheck:
      test:
        - CMD-SHELL
        - python -c "import urllib.request; urllib.request.urlopen('http://localhost:8080/health').read(); print('ok')" || exit 1
      interval: 10s
      timeout: 5s
      retries: 12
    command: |
      bash -c "
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        nohup airflow scheduler > /dev/null 2>&1 &
        exec airflow webserver
      "
    extra_hosts:
      - server:127.0.0.1

  spark:
    image: apache/spark:3.5.1
    ports:
      - '7077:7077'
      - '8081:8080'
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
    extra_hosts:
      - server:127.0.0.1

  spark-worker:
    image: apache/spark:3.5.1
    depends_on:
      - spark
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
    extra_hosts:
      - server:127.0.0.1

  flask-api:
    build:
      context: ../data-serving
      dockerfile: Dockerfile
    depends_on:
      - clickhouse
    ports:
      - '8000:8000'
    environment:
      CLICKHOUSE_URL: http://clickhouse:8123
    extra_hosts:
      - server:127.0.0.1
    profiles:
      - serving

  python:
    container_name: python
    image: python:3.11-slim
    volumes:
      - ../data-serving:/app
    environment:
      - PYTHONUNBUFFERED=1
    command: |
      bash -c "apt-get update && apt-get install -y gcc && rm -rf /var/lib/apt/lists/* &&
               pip install -U pip &&
               pip install polars==0.20.31 pyarrow pandas clickhouse-connect minio kafka-python requests &&
               tail -f /dev/null"
    extra_hosts:
      - server:127.0.0.1
    working_dir: /app

  kafdrop:
    image: obsidiandynamics/kafdrop:3.30.0
    depends_on:
      - kafka
    ports:
      - '9002:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:9092
      JVM_OPTS: '-Xms32M -Xmx64M'
    extra_hosts:
      - server:127.0.0.1

volumes:
  minio_data: null
  ch_data: null
  airflow_logs: null

networks:
  default:
    name: ${NETWORK_NAME}
    driver: bridge
